Performance engineering of code is achieved in languages that are \textit{close to the machine},\cite{Ritchie} with data types and operations that intuitively and directly correspond to hardware primitives.  A language that abstracts too far away from the hardware may be fast in many cases, but even in these good cases, there may be cycles to be gained.  Laying data out differently, possibly by packing some or all of the structure, or forcing a particular alignment, can have significant implications on performance.  Omitting instructructions added by the compiler that guarantee safety, such as bounds-checking, can be a boon when they occur inside a loop.

Consequent, and inextricably linked to this, is the need for tight control over memory allocation.  For this reason, although C and C++ have good, sturdy conservative garbage collectors\cite{BDW}(cite .NET gc), high performance applications tend to avoid it.  Garbage collectors represent a trade-off between performance and code simplicity.  This trade-off makes concurrent data structure implementation difficult in languages that are close to the machine, since most available techniques for concurrent memory reclamation require invasive modification of the operations or place restrictions on how they can be used and where pointers are allowed be stored.\cite{HP}\cite{DTA}\cite{StackTrack}\cite{Threadscan}

High performance benchmarks of concurrent data structures often forgo memory reclamation altogether.\cite{Synchrobench}\cite{Scal}  Literally, they leak the memory or use a cyclical allocator that knows about usage patterns in the benchmark and waits ``long enough'' before reallocating a chunk of memory.  Now, there's a well-reasoned argument to be made for this: Concurrent memory reclamation techniques slow down operations, and we want to know -- in the ideal case -- what is the upper bound on performance for thus-and-such a data structure?  But this upper bound isn't tight because memory reclamation is a real and valid component of performance.  Moreover, inasmuch as the benchmark code lacks memory reclamation, it probably doesn't resemble the production code; no template has been provided that can be drawn upon by application programmers.

In this work we introduce DEF, a new language designed for high performance, concurrency programming.  DEF provides primitives for traditional low-level memory management in the form of \texttt{new} and \texttt{delete}, but also adds a \texttt{retire} keyword for use with pointers that may be visible to multiple threads.  The expectation is that memory can be allocated and deallocated, just as it is in normal C programs, and \texttt{new} and \texttt{delete} can be configured to use an application-specific allocator, since performance programmers will look for the one that performs best on their application.  But when memory is shared in a concurrent data structure, \textit{invisible readers}, threads that perform nothing but read operations on a node (and are, therefore, invisible to a thread that might want to free it), memory can be retired and tracked by a special-purpose runtime.  \texttt{retire} is implemented by Forkscan that tracks nothing but retired memory and scales well across many cores.\cite{Forkscan}

As proof of concept, we implemented a benchmark suite including various concurrent data structures in DEF with corresponding (leaky) implementations in C. (Brief of results)

