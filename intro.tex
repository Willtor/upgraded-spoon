Scalability fundamentally depends upon \textit{concurrent data structures}, structures that can be used by many threads simultaneously while scaling performance accordingly.  Concurrent data structures are a well-studied topic and concurrent versions of most popular serial data structures have been discovered\cite{Harris}(cite data structures in our benchmark) and concurrency libraries even exist for some high level programming languages.\cite{JavaUtilConcurrent}  But implementing concurrent data structures in systems languages without garbage collectors is a problem unto itself, even though such languages are demanded by high performance programmers.

High performance code is coveted in a number of fields including scientific computing, computer games, machine learning, and more.  Such code is written in languages that are \textit{close to the machine}, with data types and operations that intuitively and directly correspond to hardware primitives.\cite{Ritchie}  They permit programmers to select a data layout and alignment that optimizes cache usage, omit safety instructions (such as bounds-checking), and eschew generic types that tend to add levels of indirection.

As applied to serial data structures, this is the bread-and-butter of low-level languages like C, and it's why other languages write interface guidelines for programmers to hook C modules into their applications.(citations)  As applied to concurrent data structures, however, C provides little programmer support.  The problem is one of reclaiming memory from nodes visible to multiple threads.  C has good, sturdy conservative garbage collectors\cite{BDW}\cite{DotNetGC}, but high performance applications tend to avoid them because of unpredictable delays and low scalability as compared with the multitude of other techniques C programmers apply to meet this challenge.  These techniques tend to be tailored to each structure and require invasive modification of the operations or place restrictions on where pointers are allowed to be stored and how/when they can be dereferenced.\cite{HP}\cite{DTA}\cite{StackTrack}\cite{Threadscan}  Between garbage collectors and these techniques, C provides a good case study in the present trade-off betwween peformance, and code simplicity and modularity.

Rust\cite{Rust} makes a foray into this domain with compiler-based proofs that statically determine when most memory can be deallocated.  When this is impossible, as in a concurrent data structure, Rust has tools such as \textit{reference counting} that can be employed to detect unreachable memory, but this has limited applicability.  It's slow as applied to structures that do extensive traversing (adding a write to every read), and potentially thrashes the cache in structures with nodes that tend to get touched frequently such as lists, trees, etc.  It's altogether unapplicable to structures with cycles, and hooking a Rust-implemented concurrency library into another language is made difficult by the fact that the other language needs to know about the reference counters.

In this work we introduce DEF, a language designed to integrate seemlessly into C, that provides support for high performance, concurrency programming.  Seemless integration (figure~\ref{fig:seemless-integration}) means that DEF understands C types, data structures, variables, and function declarations and can read them from C header files; and that a C header file can be automatically generated from a DEF source file, allowing C to interface with anything exported from DEF.

\begin{figure}[htbp!]
        \centering
        \includegraphics[scale=0.25]{gfx/seemless-integration}
        \caption{Integration between DEF and C source code can be done seemlessly by generating a C header file from the DEF source file, using the \texttt{defghi} utility, and importing/including the necessary header files into the respective source files.  Objects can be linked together as usual.}
        \label{fig:seemless-integration}
\end{figure}

DEF provides primitives for traditional low-level memory management in the form of \texttt{new} and \texttt{delete}, but also adds a \texttt{retire} keyword for use with pointers that may be visible to multiple threads.  The expectation is that memory can be allocated and deallocated, just as it is in normal C programs, and \texttt{new} and \texttt{delete} can be configured to use an application-specific allocator, since performance programmers generally look for one that performs best on their application.  But when memory is shared in a concurrent data structure, \textit{invisible readers}, threads that perform nothing but read operations on a node (and are, therefore, invisible to a thread that might want to free it), memory can be retired and tracked by a special-purpose runtime.

The rationale is that no tracking needs to take place on most memory in an application, and that the programmer has a good sense of when memory can \textit{probably} be deallocated (e.g., when a node is removed from the structure).  \texttt{retire} is a hint the programmer provides to the application.  As opposed to a garbage collector, that tracks and traces all memory it allocates, \texttt{retire} allows DEF to provide a more streamlined approach.

Design-wise, DEF's goal is to be good at the things C is good at, but also provide support for development of concurrent data structures.  The criteria are summarized, briefly:

\begin{enumerate}
        \item Achieve high performance on a single core, competitive with C.
        \item Facilitate development of modular concurrent data structures that expose only their interfaces to callers.
        \item Work as a drop-in replacement for C when interoperating with other programming languages.
\end{enumerate}

By default, DEF's native runtimes are the C runtime library and those that are required for support for concurrency (Forkscan\cite{Forkscan}) and parallelism (Cilk\cite{BlumofeCilk}).  Adding DEF to an existing application, therefore, creates no additional burden or potential dependency conflicts.

As proof of concept, we implemented a benchmark suite including various concurrent data structures in DEF with corresponding implementations in C.  Since C benchmarks often forgo memory reclamation altogether, or reuse memory based on knowledge of the benchmark usage patterns,\cite{Synchrobench}\cite{Scal} the C implementations in our benchmark are leaky.  (brief mention of results)
