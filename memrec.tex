In the context of concurrent data structures, modularity is defined primarily by memory management.  The API of a library is not identical to how much of its implementation it exposes.  Restrictions on where pointers can be stored or how long the caller is allowed to hold onto them are part of the profile of a library.  A modular language doesn't foist those kinds of requirements on users of libraries written in that language.

\paragraph{Managed Languages} In one respect, a garbage collector is ideal because callers can do as they please with references with the caveat that the conservative collectors of low-level languages prohibit ``hiding'' pointers with, e.g., XOR.  A caller need not understand how the data structure works, or even whether it is serial or concurrent.  For this reason, most modern languages use garbage collectors.  Even systems languages are increasingly applying GC.  The benefits are clear -- modern GC is fast with low wait times,\cite{ShahriyarBM14} impose no overhead on individual operations, and conservative collectors need not even have special type information from stack frames.

However, even among systems languages intended for high performance code, the garbage collectors have their own allocators that are non-trivial to swap out.\cite{Go, DotNetGC, D}  This is a design decision: Use GC or don't.  All heap memory or none is tracked by the garbage collector, even for objects that will never be transferred between threads.  The D programming language allows some configuration including the manual deallocation of memory allocated from the garbage collector, but all objects are still tracked from the moment of allocation.\cite{DPhobos}

Being rigid about the choice of allocator is problematic for high performance programming.  For a fixed allocator, for example, Go can advertise that they achieve comparable performance to C on some code.  But C programmers can choose from many allocators suited to different kinds of workloads, and performance can differ radically based on that choice, particularly in a multithreaded environment.\cite{Hoard, TCMalloc, JEMalloc, Supermalloc}

This relates to modularity in that a language that has its own collector, or simply a C/C++ program with a preferred allocator, can conflict with a library written in a language with a different collector in unexpected ways.  GC for DEF could never be highly integrated with the language, or it would make working with another language's collector cumbersome.

DEF's requirements differ from those of managed languages in that the assumption is that the programmer handles the memory management of an object until told otherwise via \texttt{retire}.  This usage model means that the performance of a DEF application resembles that of an unmanaged language except insofar as nodes in concurrent data structures are being retired, or are dependent upon the other language's allocator (just as C would be in the same circumstances).

There's an additional performance benefit in this interface over GC, even for the concurrent data structures, themselves.  In a concurrent data structure, retiring objects as they are removed means that they \textit{probably} can be reclaimed and reallocated, because it's unlikely that other threads are holding references to more than a few of them.  Unlike GC, few objects are being tracked, and most tracked objects aren't live.

\paragraph{Rust} The Rust programming language has its own method of safely collecting memory in a multithreaded environment without a garbage collector: all memory is ``owned'' by a single function at any given time, and the compiler can statically prove when memory can be released.\cite{Rust}  When a function is called, not only is a pointer to an object passed, but also its ownership.  If the calling function wants to use the object again, the called child must explicitly relinquish ownership when it returns.  This imposes no run time overhead, but is merely information for the compiler to use to determine when an object becomes ownerless and can be deallocated.

Though correct and fast, this limits the applicability of Rust's model in its application to concurrent data structures, themselves.  The ``interesting stuff'' (for our purposes) is precisely where many threads may be operating on a single memory location at the same time.  Such a data structure could be written in another language and imported, or the safety features in Rust could be disabled in the module, but in either case, a different means of reclaiming memory is required.  That said, a Rust-written concurrency library used within a Rust application, really is modular.  Enough information, through the type, is provided to a caller that reference counting, or whatever other means, won't fail to be honored.  This is not true when integrating the library into a non-Rust application, however.

\paragraph{Unmanaged Languages} Numerous schemes have been devised as alternatives to garbage collection for unmanaged languages.  In general, they are not automatic -- they place constraints on the programmer as to where pointers may be stored or when and how they may be dereferenced.  Nevertheless, they are successfully employed in real world applications, and are worth discussing.

\textit{Reference counting} can be applied to certain kinds of concurrent data structures\cite{DMMS, GPST09}.  Whenever a thread is ``looking at'' a shared object, it increments the object's reference counter, and decrements it again when it's done.  When the counter reaches zero, the object can be reclaimed.  This could, in principle, be automated within an unmanaged language, but it's costly to add a write on each object that's viewed, especially for structures where the most common operations are reads.  Reference counting also doesn't apply to data structures that have cycles, since removing a node with a child that references it will never be reclaimed.  And, as mentioned above, a caller into a library that uses it must know about it.

\textit{Pointer-based} mechanisms, such as Hazard Pointers\cite{HP, DTA}, typically impose lower performance penalties.  Instead of contending on shared memory, threads write to private memory, recording the objects they see.  Their private records are only viewed by other threads that actually want to free objects, so contention is lower.  There is still a write for every read, however, and they introduce memory fences to ensure their changes are visible other threads, introducing overhead per operation.  Additionally, a thread that wants to drop a reference must know that it's protected by a hazard pointer and remove it from its list, making something of the structure's implementation visible to users.

\textit{Quiescence-based} techniques\cite{FraserH07, Harris, Hart} are often performance competitive with implementations that leak memory.  These define \textit{epochs} in which references may be freely held and dropped arbitrarily.  Epochs have well-defined bounds, allowing the system to ``prove'' that there cannot be any outstanding references to nodes waiting for deletion, eliminating overhead per operation.  However, callers define epoch boundaries and prohibit the transfer of pointers across those boundaries; an exposed implementation.

In each of these cases discussed, a concurrency library written with the given technique necessarily requires users of that library to know something about how memory is tracked, and what contracts must be honored to ensure memory isn't leaked and that invalidated memory isn't dereferenced.  That's acceptable in many applications, but it isn't modular.

StackTrack\cite{StackTrack}, ThreadScan\cite{Threadscan}, and Forkscan\cite{Forkscan} are automated memory reclamation systems based on periodic searches for references.  ThreadScan and Forkscan impose no overhead on individual operations on data structures, which is particularly beneficial for obtaining maximal performance in read operations.  ThreadScan operates by periodically pausing all threads and making them search their own stacks for references.  No thread may return to work until all stacks have been searched, imposing a delay (albeit, limited in practice) until the slowest thread completes.  Additionally, no references can be permitted to escape user stacks.

Forkscan forks the process and searches for references in the resulting snapshot.  This imposes a delay on user threads while the fork takes place, but the delay isn't dependent upon the speed of any threads.  The search in the snapshot is also a complete scan of memory, so unless references are hidden (by XOR'ing or some other means), none will escape.  Like conservative GC, using a concurrency library that has Forkscan means that a caller can hold onto a reference for any amount of time and store it anywhere in memory.

For this reason, DEF uses Forkscan to satisfy the \texttt{retire} interface.  Users of libraries need not know how memory is tracked, merely that it won't leak.  Furthermore, Forkscan acts as an intermediate, transparent layer between an application and its allocator, so it permits a programmer to select the allocator that best fits the application.
