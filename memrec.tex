In the context of concurrent data structures, modularity is defined primarily by memory management.  The API of a library is not identical to how much of its implementation it exposes.  Restrictions on where pointers can be stored or how long the caller is allowed to hold onto them are part of the profile of a library.  A language's modularity is inversely correlated with the degree to which it imposes those kinds of requirements on users of libraries written in that language.

Another way to consider modularity is the ability to localize complexity.  For unmanaged languages, the appearance of concurrent data structures in the literature is deceptively simple.  An implementor of such a structure assumes that their code won't look very much like the pseudo-code provided unless they're willing to leak memory.  How much does the code need to expand in order to account for memory reclamation, and how hard is it to ``get it right?''  This may seem partially subjective, but there are practical implications for development time, the expected number of bugs, and readability and maintainability of the code.

\paragraph{Unmanaged Languages} Numerous schemes have been devised for memory management of concurrent data structures in unmanaged languages to avoid locks.  In general, they are not automatic -- they place constraints on the programmer as to where pointers may be stored or when and how they may be dereferenced.  Nevertheless, they are successfully employed in real world applications, and are worth discussing.

\textit{Reference counting} can be applied to certain kinds of concurrent data structures\cite{DMMS, GPST09}.  Whenever a thread is ``looking at'' a shared object, it increments the object's reference counter, and decrements it again when it's done.  When the counter reaches zero, the object can be reclaimed.  In low-contention, low-traversal structures this can be very fast and it could, in principle, be automated within an unmanaged language.  But it's costly to add a write on each object that's viewed if there is memory contention, or in applications where the most common operations are reads.  Reference counting also doesn't apply to data structures that have cycles, since removing a node with a child that references it will never be reclaimed.  And, as mentioned above, a caller into a library that uses it must know about it.

\textit{Pointer-based} mechanisms, such as Hazard Pointers\cite{HP, DTA, PassTheBuck}, typically impose lower performance penalties in frequently accessed objects.  Instead of contending on shared memory, threads write to private memory, recording the objects they see.  Their private records are only viewed by other threads that actually want to free objects, so contention is lower.  There is still a write for every read, however, and they introduce memory fences to ensure their changes are visible other threads, introducing overhead per operation.  Efforts have been made to avoid the use of the costly memory barrier,\cite{MauriceCadence, HazardEras} though the read-write-verify sequence can still undermine progress guarantees in data structures that possess them in the literature.\cite{Brown15}  Additionally, a thread that wants to drop a reference must know that it's protected by a hazard pointer and remove it from its list, making something of the structure's implementation visible to users.

Both reference counting and pointer-based mechanism require extensive modifications to all functions that operate on the data structures.  Even simple read operations need to broadcast their activities to other threads.

\textit{Quiescence-based} techniques\cite{FraserH07, Harris, Hart} are often performance competitive with implementations that leak memory.  These define \textit{epochs} in which references may be freely held and dropped arbitrarily.  Epochs have well-defined bounds, allowing the system to ``prove'' that there cannot be any outstanding references to nodes waiting for deletion, eliminating overhead per operation.  Complexity-wise, functions that operate on the data structures look just like they do in the literature.  However, callers define epoch boundaries and prohibit the transfer of pointers across those boundaries; an exposed implementation.  Sandboxing of this kind restricts applicability.  If a concurrent data structure contains user-provided pointers (e.g., strings as values in a hash table) that need to respect epoch boundaries, the library developer has offloaded the full complexity of concurrent memory management to the end user.

In each of these cases, a concurrency library written with the given technique necessarily requires users of that library to know something about how memory is tracked, and what contracts must be honored to ensure memory isn't leaked and that invalidated memory isn't dereferenced.  That's acceptable in many applications, but it isn't modular.

\paragraph{Managed Languages} The other extreme is garbage collection.  In one respect, a garbage collector is ideal because callers can do as they please with references with the caveat that the conservative collectors of low-level languages prohibit ``hiding'' pointers with, e.g., XOR.  A caller into a library need not understand how the data structure works, or even whether it is serial or concurrent.  For this reason, most modern languages use garbage collectors.  Even systems languages are increasingly applying GC.\cite{D, Go, Nim}  The benefits are clear -- by design, GC imposes no overhead on individual operations, conservative collectors need not have special type information from stack frames, and modern GC is fast with low wait times.\cite{ShahriyarBM14}

However, even among systems languages intended for high performance code, the garbage collectors have their own allocators that are non-trivial to swap out.\cite{Go, DotNetGC, D}  This is a design decision: Use GC or don't.  All heap memory or none is tracked, even for objects that will never be transferred between threads.  The D programming language allows some configuration including manual deallocation of memory allocated from the collector, but all objects are still tracked from the moment of allocation.\cite{DPhobos}

Being rigid about the choice of allocator is problematic for high performance programming.  For a fixed allocator, for example, Go advertises comparable performance to C on some code.  But C programmers can choose from many allocators suited to different kinds of workloads, and performance can differ radically based on that choice, particularly in a multithreaded environment.\cite{Hoard, Michael2004, TCMalloc, JEMalloc, Supermalloc}

This relates to modularity in that a language with its own collector, or simply a C/C++ program with a preferred allocator, can conflict unexpectedly with a library written in a language with a different collector.  GC for DEF could never be highly integrated with the language, or it would make working with another language's collector cumbersome.

DEF's requirements differ from those of managed languages in that the assumption is that the programmer handles the memory management of an object until told otherwise via \texttt{retire}.  This usage model means that the performance of a DEF application resembles that of an unmanaged language except insofar as nodes in concurrent data structures are being retired, or are dependent upon the other language's allocator (just as C is in those circumstances).

There's an additional performance benefit in this interface over GC, even for the concurrent data structures, themselves.  In a concurrent data structure, retiring objects as they are removed means that they \textit{probably} can be reclaimed and reallocated, because it's unlikely that other threads are holding references to more than a few of them.  Unlike GC, few objects are being tracked, and few tracked objects are live.

\paragraph{Rust} The Rust programming language has its own method of safely collecting memory in a multithreaded environment without GC: all memory is ``owned'' by a single function at any given time, and the compiler can statically prove when memory can be released.\cite{Rust}  When a function is called, not only is a pointer to an object passed, but also its ownership.  If the calling function wants to use the object again, the called child must explicitly relinquish ownership when it returns.  This imposes no run time overhead, but is merely information for the compiler to use to determine when an object becomes ownerless and can be deallocated.

Though correct and fast, this limits the applicability of Rust's model in its application to concurrent data structures, themselves.  The ``interesting stuff'' (for our purposes) is precisely where many threads may be operating on a single memory location at the same time.  Such a data structure could be written in another language and imported, or the safety features in Rust could be disabled in the module, but in either case, a different means of reclaiming memory is required.  That said, a Rust-written concurrency library used within a Rust application, really is modular.  Enough information, through the type, is provided to a caller that reference counting, or whatever other means, won't fail to be honored.  This is not true when integrating the library into a non-Rust application, however.

\paragraph{Automatic On-Demand} Automatic tracking need not be all-or-nothing.  An object can be flagged for tracking at some point during its lifetime -- after allocation.  StackTrack\cite{StackTrack}, ThreadScan\cite{Threadscan}, and Forkscan\cite{Forkscan} are automated memory reclamation systems based on periodic searches for references, but they only apply to memory explicitly designated by the programmer.  ThreadScan and Forkscan impose no overhead on individual operations on data structures, which is particularly beneficial for obtaining maximal performance in read operations.  ThreadScan operates by periodically pausing all threads and making them search their own stacks for references.  No thread may return to work until all stacks have been searched, imposing a delay (albeit, limited in practice) until the slowest thread completes.  Moreover, no references can be permitted to escape the stacks.

Forkscan forks the process and searches for references in the resulting snapshot.  This imposes a delay on user threads while the fork takes place, but the delay isn't dependent upon the speed of any threads.  The search in the snapshot is also a complete scan of memory, so unless references are hidden (by XOR'ing or some other means), none will escape.  Like conservative GC, using a concurrency library that has Forkscan means that a caller can hold onto a reference for any amount of time and store it anywhere in memory.

Conveniently, this means that a concurrent data structure using Forkscan doesn't complicate the contract the structure's implementation makes with its users.  Its modularity really is its API, just as it is with the leaky implementation.  For this reason, DEF uses Forkscan to satisfy the \texttt{retire} interface.  Users of libraries need not know how memory is tracked, merely that it won't leak.  Furthermore, Forkscan acts as an intermediate, transparent layer between an application and its allocator, so it permits a programmer to select the allocator that best fits the application.

Last, Forkscan doesn't change the lock-free properties of data structures that adopt it.  Retiring a pointer operates on a wait-free circular array unique to each thread.  And although waking the reclaimer thread takes a lock, this is essentially a rarely-accessed system lock.  The Forkscan paper observes that theoretic literature typically ignores this kind of lock.  For example, most allocators occasionally make threads acquire such a lock when accessing the global memory pool.  But programmers expect low contention on these locks and trust the code in the critical section not to hang the thread.  For all intents and purposes, a data structure doesn't lose its progress guarantees because of \texttt{malloc}.  So it is with Forkscan.

It's important to stress that \texttt{retire} is not equivalent to Forkscan.  Forkscan is the implementation, and Forkscan, itself, was adapted to fit the \texttt{retire} semantics for this work (discussed in the Implementation section).  But the principle of automatic on-demand memory reclamation is the basis for localizing the complexity of concurrent memory reclamation in high performance applications, drawing it away from data structure implementations, and certainly away from library users.  This is the value of \texttt{retire} as an idea.
