\subsection{Memory Reclamation}

\paragraph{Managed Languages} Typically, languages draw a hard line on memory allocation/deallocation.  Even among systems languages intended for high performance code, managed ones provide garbage collectors with their own allocators that are non-trivial to swap out.\cite{Go}\cite{DotNetGC}\cite{D}  This is a design decision: Use garbage collection or don't.  All memory or none is tracked by the garbage collector, even for objects that will never be transferred between threads.  The D programming language allows some configuration including the manual deallocation of memory allocated from the garbage collector, but all objects are still tracked from the moment of allocation.\cite{DPhobos}

The semantics of DEF differ from managed languages in that the assumption is that the programmer is handling the memory management of an object until told otherwise.  This usage model means that the performance of a DEF application resembles that of an unmanaged language except insofar as nodes in concurrent data structures are being retired.

There's an additional performance benefit in this interface, even for the concurrent data structures, themselves.  In a concurrent data structure, retiring objects as they are removed means that they \textit{probably} can be reclaimed and reallocated, because it's unlikely that other threads are holding references to more than a few of them.  Unlike a garbage collector, few objects are being tracked, and most tracked objects aren't live.

\paragraph{Rust} The Rust programming language has its own method of safely collecting memory in a multithreaded environment without a garbage collector: all memory is ``owned'' by a single function at any given time, and the compiler can statically prove when memory can be released.\cite{Rust}  When a function is called, not only is a pointer to an object passed, but also its ownership.  If the calling function wants to use the object again, the called child must explicitly relinquish ownership.  This imposes no run time overhead, but is merely information for the compiler to use to determine when an object becomes ownerless and can be deallocated.

Though correct and fast, this limits the applicability of Rust's model in its application to concurrent data structures, themselves.  The ``interesting stuff'' (for our purposes) is precisely where many threads may be operating on a single memory location at the same time.  Such a data structure could be written in another language and imported, or the safety features in Rust could be disabled in the module, but in either case, a different means of reclaiming memory is required.

\paragraph{Unmanaged Languages} Lack of automated tracking and collection is a feature for projects that use unmanaged languages like C or C++.  But implementing concurrent data structures in an unmanaged language requires memory reclamation, or the program will eventually crash when the system runs out of memory.  Numerous schemes have been devised as alternatives to garbage collection.  

\textit{Reference counting} can be applied to certain kinds of concurrent data structures\cite{DMMS}\cite{GPST09}.  Whenever a thread is ``looking at'' a shared object, it increments the object's reference counter, and decrements it again when it's done.  When the counter reaches zero, the object can be reclaimed.  This could, in principle, be automated within an unmanaged language, but it's costly to add a write on each object that's viewed, especially for structures where the most common operations are reads.  Reference counting also doesn't apply to data structures that have cycles, since removing a node with a child that references it will never be reclaimed.

\textit{Pointer-based} mechanisms, such as Hazard Pointers\cite{HP}\cite{DTA}, typically impose lower performance penalties.  Instead of contending on shared memory, threads write to private memory, recording the objects they see.  Their private records are only viewed by other threads that actually want to free objects, so contention is lower.  There is still a write for every read, however, and they introduce memory fences to ensure their changes are visible other threads.

\textit{Quiescence-based} techniques\cite{FraserH07}\cite{Harris}\cite{Hart} are often performance competitive with implementations that leak memory.  These define \textit{epochs} in which references may be freely held and dropped arbitrarily.  Epochs have well-defined bounds, allowing the system to ``prove'' that there cannot be any outstanding references to nodes waiting for deletion.

All of these methods of achieving memory reclamation have been applied with great success in various applications, but only reference counting can be automated, and it isn't generally applicable.  Pointer and quiescence-based systems aren't modular -- when implementing a concurrent data structure, care must be taken not to allow references to escape, or objects could be reclaimed prematurely.

StackTrack\cite{StackTrack}, ThreadScan\cite{Threadscan}, and Forkscan\cite{Forkscan} are automated memory reclamation systems based on periodic searches for references.  ThreadScan and Forkscan impose no overhead on individual operations on data structures, which is particularly beneficial for obtaining maximal performance in read operations.  ThreadScan operates by periodically pausing all threads and making them search their own stacks for references.  No thread may return to work until all stacks have been searched, imposing a delay (albeit, limited in practice) until the slowest thread completes.  Additionally, no references can be permitted to escape user stacks.  Forkscan forks the process and searches for references in the resulting snapshot.  This imposes a delay on user threads while the fork takes place, but the delay isn't dependent upon the speed of any threads.  The search in the snapshot is also a complete scan of memory, so unless references are hidden (by XOR'ing or some other means), none will escape.

\subsection{C Integration}
